{
    "Automatic Detection of Inconsistencies and Hierarchical Topic Classification for Open-Domain Chatbots": {
        "Research question": "How can we develop chatbots that provide consistent responses to semantically similar questions?",
        "Summary of introduction": "The paper proposes a methodology to address the problem of inconsistent responses in chatbots, consisting of hierarchical topic/subtopic detection using zero-shot learning and detecting inconsistent answers using clustering techniques.",
        "Dataset": "The datasets used in the study were the DailyDialog corpus and data collected by the authors' Thaurus bot during the Alexa Prize Socialbot Challenge (SGC5)",
        "Limitations": "- Expanding the range of high-level topics and evaluating the algorithm's performance on subtopic detection \n- Incorporating the topic and subtopic classifier into the dialogue management of their chatbot for the Alexa Socialbot Grand Challenge \n- Developing controllable algorithms and architectures to generate more consistent responses, and exploring ways to incorporate identified inconsistencies into automated dialogue system evaluation",
        "Research gaps": "- Expanding the range of high-level topics for the topic detection algorithm \n- Evaluating the algorithm's performance in identifying subtopics \n- Incorporating the topic and subtopic classifier into the dialogue management for the chatbot used in the Alexa Socialbot Grand Challenge (SGC5) \n- Developing controllable algorithms and architectures to generate more consistent responses, leveraging persona profiles \n- Exploring mechanisms to incorporate identified inconsistencies into an automated evaluation of dialogue systems",
        "Software used": "The software and computer programs used in the study include: \n- DailyDialog dataset \n- Alexa Socialbot Grand Challenge 5 (SGC5) data \n- GPT-4 for generating questions and paraphrases \n- Intel Core i9-10900F CPU \n- NVIDIA GeForce RTX 3090 GPU with CUDA 11.4",
        "Algorithms": "The two main algorithms introduced in the paper are: \n1) A hierarchical zero-shot learning algorithm for topic and subtopic classification \n2) An automated algorithm to detect inconsistencies in chatbot responses",
        "Methodology": "The methodology consists of two main parts: \n1. Hierarchical topic and subtopic detection using a zero-shot learning approach based on natural language inference (NLI): \n- They use a pre-trained NLI model to classify the topic and subtopic of dialogue turns \n- They do this in a hierarchical way, first detecting the high-level topic and then the more specific subtopic \n- This allows them to handle a large number of topics and subtopics without needing to retrain the model \n2. Automatic detection of inconsistent chatbot responses using clustering techniques: \n- They identify clusters of similar questions using an unsupervised clustering approach \n- They then analyze the responses to those similar questions to detect cases where the chatbot provides inconsistent responses",
        "Main findings": "- The authors propose a novel methodology for hierarchical automatic detection of topics and subtopics in dialogue interactions using a zero-shot learning approach. \n- The authors also propose a method for detecting inconsistent answers using k-means and the Silhouette coefficient. \n- The authors report experimental results showing high performance for topic and subtopic detection, as well as the ability to predict the number of different responses from chatbots with low mean squared error.",
        "Study Objectives": "1. Develop a hierarchical algorithm for scalable and adaptable topic and subtopic classification using zero-shot learning \n2. Develop an algorithm to automatically detect inconsistent responses in chatbots",
        "Study design": "Not applicable (the paper is focused on developing and evaluating algorithms for topic detection and inconsistency detection in chatbots, rather than describing a traditional study design)",
        "Intervention effects": "- Topic detection weighted F1 score: 0.34 \n- Subtopic detection weighted F1 score: 0.78 \n- Topic detection accuracy on SGC5: 81% \n- Subtopic detection accuracy on SGC5: 62% \n- Mean squared error in predicting number of different responses: \n- Smaller generative models: 3.4 \n- Recent large language models: 4.9",
        "Hypotheses tested": "- The paper proposes two main hypotheses: \n1) Chatbots can provide different answers to semantically similar questions due to factors like training data, dialogue history, decoding mechanisms, and ranking strategies. \n2) The paper proposes a methodology to (a) hierarchically detect topics and subtopics using zero-shot learning, and (b) detect inconsistent answers using k-means and the Silhouette coefficient.",
        "Experimental techniques": "- Using the DailyDialog dataset and real dialogue interactions from the Alexa Socialbot Grand Challenge 5 to evaluate topic and subtopic detection \n- Manually generating paraphrased questions and using pre-trained chatbot models to generate responses, in order to detect inconsistencies \n- Using GPT-4 to automatically generate new topic-based questions and paraphrases, which were then manually annotated by experts, to further evaluate the inconsistency detection algorithm"
    },
    "Cicognini at ACTI: Analysis of techniques for conspiracies individuation in Italian": {
        "Research question": "How can we develop effective automatic models to detect and classify conspiratorial content in Italian language online communications?",
        "Summary of introduction": "The paper focuses on methods and results for solving the EVALITA 2023 ACTI challenge, which involves classifying Italian messages as conspiratorial or not, and if so, what type of conspiracy they are about, in the context of the increasing prevalence of conspiratorial content online and the challenges of content moderation.",
        "Dataset": "The datasets used in the study are: \n1) For Subtask A: A dataset of 1,842 labeled Italian Telegram messages, with a binary label indicating whether the message is conspiratorial or not. There is also a hidden test set of 460 samples. \n2) For Subtask B: A dataset of 810 labeled Italian Telegram messages, with a label from 0 to 3 indicating the conspiracy topic (COVID-19, QAnon, Flat Earth, Pro-Russia). There is also a hidden test set of 300 samples.",
        "Limitations": "Not mentioned (the paper does not discuss any limitations of the study)",
        "Research gaps": "No research gaps suggested",
        "Software used": "Python, PyTorch, Scikit-Learn, Transformers",
        "Algorithms": "The specific algorithms introduced, studied, or used in this study are: \n- Transformer-based models: BERT-xxl, XLM-RoBERTa, LLaMA \n- Topic-specific TF-IDF approach",
        "Methodology": "The methodology used in the study includes: \n- Transformer-based models (BERT-xxl, XLM-RoBERTa, LLaMA) for classifying messages as conspiratorial or not (Subtask A) and for classifying the type of conspiracy (Subtask B) \n- An original heuristic baseline using topic-specific TF-IDF for Subtask B, where the most relevant keywords for each conspiracy topic are identified and fed into a Random Forest classifier",
        "Main findings": "- The transformer-based models, specifically BERT-xxl and XLM-RoBERTa, achieved top performance scores over 80% on both subtasks. \n- The size of the transformer model (number of parameters) did not significantly impact the performance. \n- Combining the transformer-based models and the Topic-specific tf-idf model could result in a more robust model.",
        "Study Objectives": "The study objectives are: \n- Solve SubtaskA: Conspiracy detection \n- Solve SubtaskB: Conspiracy topic classification",
        "Study design": "Not mentioned (the paper does not provide details about the study design beyond the use of a hold-out validation set)",
        "Intervention effects": "Not applicable (the paper does not report any intervention effects)",
        "Hypotheses tested": "Not mentioned (the paper does not state any specific hypotheses to be tested)",
        "Experimental techniques": "- Use of transformer-based models like BERT-xxl, XLM-RoBERTa, and LLaMA for text classification \n- Development of an original heuristic baseline using topic-specific TF-IDF and a Random Forest classifier"
    },
    "Query-Focused Submodular Demonstration Selection for In-Context Learning in Large Language Models": {
        "Research question": "How can we develop better methods for selecting demonstration examples to improve the performance of in-context learning in large language models?",
        "Summary of introduction": "The paper introduces in-context learning (ICL) in large language models, which allows models to perform tasks based on instructions and demonstration examples, and proposes using query-focused submodular mutual information functions to select diverse and representative demonstration examples, as well as an interactive tool to explore the impact of hyperparameters on model performance in ICL.",
        "Dataset": "SST-2, SST-5, and AgNews",
        "Limitations": "The paper does not explicitly state any limitations of the study. \nHowever, it mentions areas for future research and potential improvements, such as: \n- Exploring specialized data selection methods using submodular information functions to choose diverse and relevant demonstration examples. \n- Developing an interactive tool to adjust various hyperparameters in in-context learning. \n- Noting that larger models do not always outperform smaller models, and that parameter count alone does not guarantee superior performance.",
        "Research gaps": "- The methods for choosing effective demonstration examples to enhance predictive performance in in-context learning (ICL) are not fully understood, with prior research often relying on random selection. \n- ICL's effectiveness can be significantly enhanced through adaptation during pre-training. \n- ICL's performance is notably influenced by the choice and arrangement of in-context examples.",
        "Software used": "The paper mentions the authors developed an interactive tool using various software and programming languages, but does not specify the exact software or languages used.",
        "Algorithms": "Submodular functions, greedy algorithms, Submodular Mutual Information (SMI) functions (including Graph Cut MI, Facility Location MI, and Log Determinant MI)",
        "Methodology": "1. Training a retriever model using sentence embeddings to evaluate similarities \n2. Applying submodular mutual information (SMI) functions to select a subset of k diverse and relevant demonstration examples\n 3. Integrating the k demonstration examples into a prompt template \n4. Feeding the prompt template into a pre-trained language model to predict the label using perplexity",
        "Main findings": "1) Using query-focused submodular functions to select demonstration examples for in-context learning can lead to performance improvements of up to 20% compared to random selection or traditional prompting methods. \n2) The paper introduces an interactive tool that allows users to experiment with various hyperparameters related to in-context learning, such as the quantity and generation methods of demonstration examples. \n3) Contrary to initial assumptions, the size and type of the language model do not always guarantee better performance, as smaller models sometimes outperformed larger alternatives in the same model family.",
        "Study Objectives": "- To use query-focused submodular mutual information functions to select diverse and representative demonstration examples for in-context learning, in order to improve few-shot performance compared to random and zero-shot baselines. \n- To introduce an interactive tool to explore the impact of hyperparameters on model performance, including the quantity and generation methods of demonstration examples, and their influence on data manifolds and clusters.",
        "Study design": "Not applicable (the paper is focused on developing a method, not describing a traditional study design)",
        "Intervention effects": "Not applicable (the paper does not report quantitative effects of any interventions, but rather compares the performance of different methods for selecting demonstration examples in in-context learning)",
        "Hypotheses tested": "- Using query-focused submodular functions to select diverse and representative demonstration examples will improve the performance of in-context learning compared to random selection or traditional prompting methods. \n- The effectiveness of submodular functions for selecting demonstration examples is task-dependent, and there is no universal function suitable for all datasets. \n- The size and type of the language model used for inference does not always guarantee better performance, and smaller models can sometimes outperform larger ones.",
        "Experimental techniques": "- Selecting demonstration examples using query-focused submodular functions, including: \n- Facility MI \n- Graph Cut MI \n- Log Determinant MI \n- Using sentence embeddings to represent the instances and evaluate similarity kernels \n- Optimizing a mutual information function I_f(A;T) to identify the best subset of size k from the training set U, with the test set T as the reference"
    },
    "MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model": {
        "Research question": "How can transit agencies develop transit-specific natural language models and tools to efficiently and meaningfully analyze unstructured customer feedback from various channels, including CRM and social media, to understand trends and improve customer experience?",
        "Summary of introduction": "The paper proposes leveraging traditional transit CRM feedback to develop a transit-topic-aware language model that can classify open-ended text feedback into relevant transit-specific topics, in order to enable transit agencies to better understand and analyze customer feedback at scale.",
        "Dataset": "The main dataset used in the study is a dataset containing around 180,000 customer feedback comments with manually-classified labels from WMATA's CRM database, covering the period from January 2017 to December 2022. The study also used a second dataset containing around 300,000 tweets mentioning WMATA's official Twitter handles during the same period.",
        "Limitations": "- LDA was unable to detect niche topics that were less represented, such as complaints about crime, harassment, and safety \n- Around 35% of complaints did not have a significant primary topic score and were left unassigned \n- Using LDA to generate ground truth labels for the training dataset can lead to errors due to the lexicon-based nature of topic clustering",
        "Research gaps": "- The LDA topic modeling approach was unable to detect niche topics that were less represented in the data, such as complaints about crime, harassment, and safety. \n- The TF-IDF feature matrix used for the traditional machine learning models had limitations, including not handling out-of-vocabulary terms and not considering context. \n- The authors suggest future work to create sub-models to break down the broad topics detected by MetRoBERTa into more granular topics.",
        "Software used": "RoBERTa, RoBERTa-based sentiment model",
        "Algorithms": "The specific algorithms introduced, studied, or used in the study were: \n1. Latent Dirichlet Allocation (LDA) to detect 23 broad transit-specific topics from the customer feedback data \n2. RoBERTa, a large language model, which was retrained on the dataset as described in Algorithm 2 to create a transit-topic-aware language model called MetRoBERTa",
        "Methodology": "1. Created a TF-IDF feature matrix from the CRM complaint data, using unigrams and bigrams that appear at least 20 times, after accounting for stop words and WMATA-specific terms. \n2. Trained and evaluated 5 traditional machine learning models (Random Forest, Linear Regression with SGD, SVM, Naive Bayes, Logistic Regression) using the TF-IDF feature matrix. \n3. Trained a RoBERTa-based large language model (called MetRoBERTa) on the CRM complaint data, as described in Algorithm 2.",
        "Main findings": "- The MetRoBERTa language model outperforms traditional machine learning models in classifying transit-related topics in customer feedback, with an average accuracy of at least 90%. \n- The MetRoBERTa model is able to correctly identify misclassifications in the training data, which were due to the manual nature of assigning problem categories or the keyword-based topic modeling approach. \n- The authors provide a value proposition for using the MetRoBERTa model, along with additional text processing tools, to add structure to open-ended customer feedback sources like Twitter, and expect the model's performance to improve with more training data.",
        "Study Objectives": "The key objectives of the study were to: \n1) Develop a framework for detecting latent transit-specific topics in transit customer feedback data, \n2) Classify unstructured transit customer feedback into those transit-specific topics, \n3) Evaluate the accuracy of the resulting transit-topic-aware natural language model, and \n4) Demonstrate the application of the model on unstructured customer feedback from Twitter and re-classify feedback from existing CRM channels.",
        "Study design": "The study design is a retrospective, non-controlled observational study that develops and evaluates a natural language processing model to classify transit-related customer feedback into relevant topics.",
        "Intervention effects": "Not applicable (the paper is focused on the development and evaluation of a language model, and does not report any quantitative intervention effects)",
        "Hypotheses tested": "Not mentioned (the paper does not state a specific hypothesis that is being tested)",
        "Experimental techniques": "- Creating a TF-IDF feature matrix from the CRM complaint data, using unigrams and bigrams that appeared at least 20 times \n- Training and evaluating 5 traditional machine learning models (Random Forest, Linear Regression with SGD, SVM, Naive Bayes, Logistic Regression) on the TF-IDF feature matrix \n- Training a RoBERTa-based large language model (MetRoBERTa) on the CRM complaint data"
    },
    "Semi-Automatic Topic Discovery and Classification for Epidemic Intelligence via Large Language Models": {
        "Research question": "How can we develop a framework to use large language models for epidemic intelligence to identify and categorize emergent socio-political phenomena during health crises?",
        "Summary of introduction": "The paper introduces a novel framework that uses Large Language Models to identify and categorize emergent socio-political phenomena during health crises, such as the COVID-19 pandemic, and provides explicit support to analysts through the generation of distinct thematic areas and actionable statements for a Zero-shot Classification mechanism.",
        "Dataset": "The dataset used in the study consists of 2,254 news articles manually categorized by ISS experts into 5 topics: \"Covid Variants\", \"Nursing Homes Outbreaks\", \"Hospital Outbreaks\", \"School Outbreaks\", and \"Family/Friend Outbreaks\", collected during the period from February 2020 to September 2022.",
        "Limitations": "- The system is not a fully automated black-box approach, but rather intended to enhance the analyst's capabilities through interactive collaboration \n- The use of a decoder LLM faces potential limitations such as susceptibility to hallucinations, which could lead to inaccurate descriptions of emerging topics and prompts",
        "Research gaps": "No research gaps suggested",
        "Software used": "Google News, a custom crawling service",
        "Algorithms": "The key algorithm introduced in this study is the Linguistic Triple Generation process, where the system generates structured Subject-Verb-Object (SVO) triples from seed terms and uses these triples to discover topics and generate prompts for zero-shot classification.",
        "Methodology": "The methodology consists of three main steps: \n1) Data Gathering: Analysts provide seed terms to retrieve a corpus of relevant news articles; \n2) Topic Discovery: The system generates linguistic triples to capture fine-grained concepts, which analysts can refine; \n3) Zero-shot Classification: The system applies zero-shot classification using large language models to match news articles to the identified topics, without requiring model fine-tuning.",
        "Main findings": "- It introduces a novel framework that uses Large Language Models (LLMs) to identify and categorize emergent socio-political phenomena within health crises, particularly the COVID-19 pandemic. \n- This approach provides explicit support to analysts by identifying distinct thematic areas and generating clear, actionable statements for each topic, enabling effective zero-shot classification of news articles. \n- Preliminary results show that this workflow can accurately map news articles to pertinent fine-grain topics.",
        "Study Objectives": "- Introduce a novel framework that uses Large Language Models (LLMs) for Epidemic Intelligence \n- Focus on identifying and categorizing emergent socio-political phenomena within health crises, with a focus on the COVID-19 pandemic \n- Ensure each analytical step is self-explanatory and elucidate the rationale behind the interpretation for analysts not familiar with the subject matter",
        "Study design": "Not mentioned (the paper does not describe a specific study design)",
        "Intervention effects": "Not mentioned (the paper does not report any quantitative intervention effects)",
        "Hypotheses tested": "Not mentioned (the paper does not state a specific hypothesis that is being tested)",
        "Experimental techniques": "- Assigning scores to news articles for positive and negative classes \n- Defining Zero-shot classification policies: Best Probability, Top-Best Average, Topic Average, Class Average \n- Assessing the system's ability to reclassify individual news articles and rank the classes based on the scores"
    },
    "ACTI at EVALITA 2023:Automatic Conspiracy Theory Identification Task Overview": {
        "Research question": "How can we develop better methods to automatically identify and classify conspiracy theory content on online platforms?",
        "Summary of introduction": "The introduction provides background on the prevalence of conspiracy theories throughout history, how the internet has become a prominent medium for their spread, and the real-world impacts of online conspiracy theories, setting the stage for the ACTI task which aims to address the challenge of accurately identifying conspiratorial content online.",
        "Dataset": "The dataset used in the study is a collection of 25,612 Italian comments scraped from 5 Telegram channels known for hosting conspiratorial content, collected between January 1, 2020, and June 30, 2020. The comments were manually annotated by two human annotators to identify conspiratorial content and categorize it into specific conspiracy theories.",
        "Limitations": "- Presence of non-conspiratorial content within the channels, which needed to be filtered out \n- Need to employ human annotators to label the comments as \"Not Relevant\", \"Non-Conspiratorial\", or \"Conspiratorial\" \n- Need to exclude comments that did not receive the same classification from both annotators, in order to maintain data integrity \n- Exclusion of comments labeled as \"Not Relevant\" to focus solely on relevant conspiratorial content",
        "Research gaps": "- The need for fundamental technology to accurately identify conspiratorial content at scale across languages and cultural contexts. \n- The need to refine prompting techniques to improve the predictive capacity of LLMs on specific tasks.\n- The need to assess the authenticity and alignment of synthetic data generated by LLMs with real-world conspiratorial beliefs, and the need for human evaluation of the effectiveness of these approaches.",
        "Software used": "Python, Selenium, BeautifulSoup",
        "Algorithms": "The specific algorithms introduced, studied, or used in the study were: \n1) Random Forest models trained on bag-of-words representations of the comments (used as baselines) \n2) Prompting of large language models (LLMs) \n3) Data augmentation using LLMs and training of sentence transformers with contrastive learning (used by the winning team)",
        "Methodology": "The methodology used in the study involved: \n- Employing a customized web crawler to target Telegram channels known for hosting conspiratorial content \n- Using two human annotators to label the comments as \"Not Relevant\", \"Non-Conspiratorial\", or \"Conspiratorial\" \n- Further categorizing the \"Conspiratorial\" comments into four subcategories: \"QAnon\", \"Covid19\", \"Russia\", and \"Flat-Earth\" \n- Calculating inter-annotator agreement rates using Cohen's coefficient to ensure the reliability of the annotation process",
        "Main findings": "- The paper discusses the Automatic Conspiracy Theory Identification (ACTI) task, which involves identifying conspiratorial content and classifying it into specific conspiracy theories. \n- The paper analyzes the approaches used by the participating teams, noting that most teams used large language models (LLMs), with two key approaches being prompting and data augmentation. \n- The paper finds that while prompting can produce positive results, the predictive capabilities of zero-shot LLMs are still inferior to models that have been fine-tuned for the specific task, and that validating the quality of synthetic data generated by LLMs is an open issue that requires human evaluation.",
        "Study Objectives": "- Identify conspiratorial content in comments from conspiratorial Telegram channels (conspiratorial content classification) \n- Classify the content into specific conspiracy theories (conspiratorial category classification)",
        "Study design": "The study design is a retrospective observational study, where the researchers collected and analyzed existing data from Telegram channels known for hosting conspiratorial content. The study involved a data collection process using a web crawler, followed by human annotation of the collected comments into \"Not Relevant\", \"Non-Conspiratorial\", and \"Conspiratorial\" categories, with further subcategorization of the \"Conspiratorial\" comments.",
        "Intervention effects": "Not applicable (the paper does not report any quantitative intervention effects)",
        "Hypotheses tested": "Not applicable (the paper does not state a specific hypothesis to be tested, but rather describes the setup of a new shared task)",
        "Experimental techniques": "- Customized web crawler using Selenium and BeautifulSoup libraries in Python to gather data from Telegram channels \n- Human annotation of comments as \"Not Relevant\", \"Non-Conspiratorial\", or \"Conspiratorial\", with further categorization of \"Conspiratorial\" comments into \"QAnon\", \"Covid19\", \"Russia\", and \"Flat-Earth\" \n- Calculation of inter-annotator agreement using Cohen's kappa, with high agreement levels of 0.93 and 0.86 for the two tasks"
    },
    "LAraBench: Benchmarking Arabic AI with Large Language Models": {
        "Research question": "Can large language models effectively perform Arabic NLP and speech processing tasks without prior task-specific knowledge, and how do their performances compare to state-of-the-art models?",
        "Summary of introduction": "LAraBench addresses the gap in benchmarking large language models against state-of-the-art models for Arabic natural language processing and speech processing tasks.",
        "Dataset": "61 publicly available datasets",
        "Limitations": "Performance gap between LLMs and SOTA models, especially in zero-shot settings and for syntactic tasks \n- Dependence of model performance on effective prompting and post-processing techniques \n- Larger performance gap between MSA and dialectal datasets, indicating ineffectiveness of LLMs for under-represented dialects \n- Better performance of LLMs on semantic tasks compared to syntactic tasks \n- Potential contamination of evaluation datasets in the training of LLMs",
        "Research gaps": "No research gaps suggested",
        "Software used": "GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13bchat, Whisper, USM, Amazon Polly",
        "Algorithms": "Zero-shot and few-shot learning",
        "Methodology": "- The study used zero-shot and few-shot learning approaches to evaluate the performance of large language models (LLMs) on Arabic NLP and speech processing tasks. \n- The LLMs evaluated include GPT-3.5-Turbo, GPT-4, BLOOMZ, and Jais-13b-chat for NLP tasks, and Whisper, USM, and Amazon Polly for speech tasks. \n- The researchers explored various prompts to guide the LLMs in performing the tasks, following recommendations from Azure OpenAI Studio Chat playground and PromptSource. \n- For the zero-shot setup, the researchers used natural language instructions to describe the tasks and specify the expected output. \n- For the few-shot setup, the researchers used available training data to select a few relevant examples and provide context to the LLMs. \n- Post-processing of the LLMs' outputs was necessary to enable automatic comparison with gold standard labels, which involved mapping prefixes, filtering tokens, and other task-specific adjustments.",
        "Main findings": "- GPT-4 outperforms other LLMs in most NLP tasks, but still lags behind SOTA models. \n- GPT-4 can significantly reduce the performance gap with SOTA models when using just 3-shot learning, especially for more complex semantic and QA tasks. \n- LLMs perform much worse on dialectal Arabic datasets compared to MSA, indicating they struggle with under-represented dialects.",
        "Study Objectives": "- Assess the capabilities of large language models (LLMs) like GPT-3.5-turbo, GPT-4, Jais-13bchat, BLOOMZ, Whisper, and USM on Arabic NLP and speech processing tasks \n- Investigate whether LLMs can effectively perform these tasks in zero-shot settings without prior task-specific knowledge \n- Examine how LLM performance varies across tasks with different complexities in zero-shot and few-shot settings \n- Compare the performance of LLMs to current state-of-the-art (SOTA) models, and assess whether open LLMs are as effective as commercially available (closed) models",
        "Study design": "The study design appears to be an empirical evaluation study that benchmarks the performance of large language models (LLMs) on 33 distinct NLP and speech processing tasks across 61 publicly available datasets, using zero-shot and few-shot learning techniques. The study does not mention any specific type of experimental design (e.g., randomized, controlled, etc.), as it is focused on a comparative evaluation of model performance.",
        "Intervention effects": "Not applicable (the paper does not report quantitative intervention effects, but rather compares the performance of different language models on various NLP and speech processing tasks)",
        "Hypotheses tested": "- Can LLMs effectively perform Arabic NLP and speech processing tasks without prior task-specific knowledge (zero-shot)? \n- How does performance vary across tasks with different complexities in zero-and few-shot settings?\n- How do LLMs compare to current SOTA models, and are open LLMs as effective as the commercially available (closed) models?",
        "Experimental techniques": "- Zero-shot and few-shot learning techniques \n- GPT-3.5-Turbo, GPT-4, BLOOMZ, and Jais-13b-chat for NLP tasks \n- Whisper (small, medium, and large), USM, and Amazon Polly for speech tasks \n- Prompting using instructions from Azure OpenAI Studio Chat playground and PromptSource \n- Using modality-specific API services (OpenAI API for NLP, Google's USM API for speech) \n- Using on-premises hosted versions of BLOOMZ and Jais-13b-chat"
    },
    "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs": {
        "Research question": "How can large language models be leveraged to enhance the automatic analysis of public affairs documents?",
        "Summary of introduction": "Large Language Models have the potential to greatly enhance the analysis of public affairs documents, which is crucial for citizens and companies to promote transparency, accountability, and informed decision-making.",
        "Dataset": "The dataset used in the study is a corpus of over 33,000 multi-label public affairs documents from the Spanish Parliament, annotated with 30 different topics.",
        "Limitations": "- Studying biases and imbalances in the dataset in more depth, and compensating for them using imbalance-aware machine learning procedures \n- Testing more recent LLMs, including multilingual and instruction-based models, which have shown great capabilities in various NLP tasks \n- Exploring the incorporation of other NLP tasks (e.g., text summarization, named entity recognition) and multimodal methods to the framework, to further enhance the automatic analysis of public affairs documents",
        "Research gaps": "- Studying biases and imbalances in the dataset, and compensating for them using imbalance-aware machine learning procedures \n- Testing more recent large language models, including multilingual and instruction-based models, for the task of topic classification in public affairs documents \n- Exploring the incorporation of other NLP tasks (e.g. text summarization, named entity recognition) and multimodal methods to the framework, to enhance the automatic analysis of public affairs documents",
        "Software used": "HuggingFace transformers library, sklearn tools",
        "Algorithms": "The specific algorithms introduced, studied, or used in this study include: \n- Transformer-based language models: RoBERTa-base, RoBERTa-large, RoBERTalex, and GPT2-base \n- Classifiers: Neural networks, random forests, and support vector machines (SVMs) \n- Binary neural network classification output layer \n- Weighted cross-entropy loss \n- One-vs-all setup for multi-label classification \n- Using [CLS] token embedding or averaging all token embeddings as input to the classifiers \n- Hyperparameter tuning for each topic",
        "Methodology": "The methodology used in this study is a multi-label topic classification approach, where a binary topic classifier is trained for each of the 30 most frequent topics in the dataset. The architecture of the binary topic classifiers consists of a transformer-based model as the backbone, followed by a neural network, random forest, or SVM classifier. The transformer models used include RoBERTa-base, RoBERTa-large, RoBERTalex, and GPT2-base, which were pretrained on Spanish text. The transformer models were trained with a binary neural network classification output layer, using weighted cross-entropy loss to address class imbalance. For the SVM and random forest classifiers, the authors used RoBERTa-base to extract text embeddings, either from the [CLS] token or by averaging all token embeddings. The SVM and random forest classifiers were trained with the RoBERTa-base embeddings, using a complexity parameter of 1 and RBF kernel for the SVM, and a max depth of 1,000 for the random forest.",
        "Main findings": "- Using a Large Language Model (LLM) backbone in combination with Support Vector Machine (SVM) classifiers is an effective strategy for multi-label topic classification in the domain of public affairs, achieving accuracies over 85%. \n- The SVM classification approach improves accuracies consistently, even for topics with a lower number of samples (less than 500 samples).",
        "Study Objectives": "- To develop new tools that allow citizens and businesses to quickly access regulatory changes that affect their present and future operations \n- To propose, develop, and evaluate a novel functionality for multi-label topic classification within a general document analysis system for public affairs documents",
        "Study design": "Not mentioned (the paper does not provide details on the overall study design, as it is focused on describing the topic classification component of a larger research project)",
        "Intervention effects": "Not applicable (the paper does not report any quantitative effects of an intervention)",
        "Hypotheses tested": "- The project aims to improve the automatic analysis of public affairs documents using advancements in Document Layout Analysis and Language Technologies. \n- The paper proposes and evaluates a novel functionality for multi-label topic classification within a general document analysis system for public affairs documents. \n- The paper presents a new dataset of public affairs documents annotated by topic, which represents the main Spanish legislative activity between 2019 and 2022.",
        "Experimental techniques": "- K-fold cross validation with 5 folds - Binary neural network classification with weighted cross-entropy loss \n- One-vs-all topic classification setup \n- Training for 5 epochs with batch size of 32 and frozen transformer layers \n- Using [CLS] token embedding or mean pooling of token embeddings as input to SVM and Random Forest classifiers \n- SVM with complexity parameter of 1 and RBF kernel - Random Forest with max depth of 1,000"
    },
    "Improving short text classification with augmented data using GPT-3": {
        "Research question": "How can we construct a better set of training examples to improve GPT-3's few-shot learning capabilities?",
        "Summary of introduction": "This paper explores using data augmentation to improve the performance of GPT-3 on a short text classification task, specifically classifying whether a question is related to data science, by comparing two approaches: augmenting the GPT-3 Classification Endpoint by increasing the training set size, and augmenting the GPT-3 Completion Endpoint by optimizing the prompt using a genetic algorithm.",
        "Dataset": "The dataset used in the study consists of 72 short text questions collected from the University of Massachusetts Dartmouth Big Data Club's Discord server, with 45 questions collected directly from Discord messages and 27 additional questions proposed by club members and edited by the research team to cover a broad range of data science topics.",
        "Limitations": "- The results are confined to the English language, and further work is needed to expand the method to other languages. \n- The sample size of the validation and test sets is small, which reduces the robustness of the results. \n- The study is limited to a single classification problem, and further research is needed on different types of classification problems. \n- The set of possible questions is broad, and the impact of including a subset of 27 questions to cover a wide variety of topics is unclear. \n- The paper suggests that future work could include additional label categories to handle cases where there is disagreement among annotators on the true topic of a question.",
        "Research gaps": "- Evaluating data augmentation on different types of classification problems, such as those with multiple classes or class imbalance \n- Assessing model performance in even more limited-data scenarios (e.g. 4-5 labeled examples) \n- Incorporating additional label types to handle subjective labeling (e.g. \"Unsure\", \"Sure Data\", \"Possibly Data\") \n- Exploring alternative optimization techniques beyond genetic algorithms for selecting training examples",
        "Software used": "Python 3.8.5, OpenAI Python API",
        "Algorithms": "Not mentioned (the paper does not introduce any specific new algorithms)",
        "Methodology": "- Classification Endpoint Augmentation: Generating new questions using GPT-3 and adding them to the training set for the Classification Endpoint \n- Completion Endpoint Augmentation: Using a genetic algorithm to select optimal generated questions to include in the prompt for the Completion Endpoint",
        "Main findings": "- Data augmentation using GPT-3 significantly increased the accuracy of both the GPT-3 Classification Endpoint and Completion Endpoint. \n- The embedding-based GPT-3 Classification Endpoint achieved the best accuracy of about 76%, compared to estimated human accuracy of 85%. \n- The GPT-3 Completion Endpoint with a genetic algorithm for optimizing in-context examples achieved high validation accuracy but lower test accuracy, indicating potential overfitting.",
        "Study Objectives": "- Evaluate two data augmentation methods using GPT-3 to improve short text classification: \n1) Augment the GPT-3 Classification Endpoint by generating additional training examples \n2) Augment the GPT-3 Completion Endpoint by using a genetic algorithm to select optimal generated training examples",
        "Study design": "Not mentioned (the paper does not explicitly state the study design)",
        "Intervention effects": "Not mentioned (the paper does not report any quantitative intervention effects, only the accuracy of the classification models)",
        "Hypotheses tested": "- Using GPT-3 to generate new examples can improve the performance of the GPT-3 Classification Endpoint for short text classification. \n- Using a genetic algorithm to select optimal generated examples can improve the performance of the GPT-3 Completion Endpoint for short text classification.",
        "Experimental techniques": "- Augmenting the GPT-3 Classification Endpoint by generating new questions using GPT-3 and adding them to the training set \n- Optimizing the training examples used in the GPT-3 Completion Endpoint using a genetic algorithm"
    },
    "ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-Resource Language NLP Tasks": {
        "Research question": "How reliable and applicable are LLM-generated annotations compared to human-generated annotations in Turkish, Indonesian, and Minangkabau NLP tasks, and when should LLM-generated annotations be used versus human expertise?",
        "Summary of introduction": "This research paper presents a comparative study on the quality of annotations generated by human annotators and Large Language Models (LLMs) for Turkish, Indonesian, and Minangkabau NLP tasks, with the primary motivations of contributing to the discourse on the reliability of LLM-generated annotations in these low-resource languages and addressing the practical implications of selecting annotation sources in the era of LLMs.",
        "Dataset": "The datasets used in the study include: \n- 3 Turkish datasets: Dataset for Topic Classification (DTC), Dataset for Tweet Sentiment Analysis (DTSA), and Dataset for Emotion Classification (DEC) \n- 2 Indonesian datasets: Indonesian Dataset for Tweet Sentiment Analysis (IDTSA) and Indonesian Dataset for Emotion Classification (IDEC) \n- 2 Minangkabau datasets: Minangkabau Dataset for Tweet Sentiment Analysis (MDTSA) and Minangkabau Dataset for Emotion Classification (MDEC), translated from the Indonesian datasets",
        "Limitations": "- Limitations of LLMs in context comprehension, ambiguity resolution, and domain-specific nuances \n- Challenges in achieving high-quality annotations, requiring a balance of linguistic expertise, domain knowledge, and meticulous guidelines \n- Constraints of human annotation, including cost, scale, and consistency",
        "Research gaps": "- Lack of studies on NLP tasks in Turkish, Indonesian, and Minangkabau languages \n- Lack of comprehensive assessment of the quality of LLM-generated annotations \n- Lack of comparison between LLMs and human experts in annotation quality",
        "Software used": "Human annotation, Mechanical Turk (MTurk), ChatGPT-4, BERT, RoBERTa, T5",
        "Algorithms": "The specific algorithms introduced, studied, or used in the study were: ChatGPT-4, BERT (specifically the BERTurk variant), RoBERTa, and GPT-3. These large language models were used to generate annotations for the various NLP tasks in the study.",
        "Methodology": "- Data selection: The study used three distinct Turkish datasets, two Indonesian datasets, and two Minangkabau datasets, each tailored for specific NLP tasks (topic classification, tweet sentiment analysis, and emotion classification). \n- Annotation process: The study employed three annotation methodologies: \n1. Human annotation: Human annotators, proficient in the target languages, followed comprehensive annotation guidelines. \n2. Mechanical Turk (MTurk) annotation: Crowd-workers on the MTurk platform followed the same annotation guidelines as the human annotators. \n3. LLM annotation: Large Language Models (LLMs), including ChatGPT-4, BERT, RoBERTa, and T5, were used to generate annotations, guided by task-specific input prompts and fine-tuned for linguistic and cultural relevance. \n- Task-specific guidelines: The study developed comprehensive guidelines for each NLP task, considering the nuances of the target languages (Turkish, Indonesian, and Minangkabau).",
        "Main findings": "- Human annotations consistently outperformed LLMs across various metrics, establishing them as the benchmark for annotation quality. \n- ChatGPT-4 and BERTurk showed competitive performance, but still lagged behind human annotations in certain aspects. \n- There was a trade-off between precision and recall observed across the LLMs, indicating room for improvement in maintaining a balance between the two.",
        "Study Objectives": "1. To contribute to the understanding of the reliability and applicability of LLM-generated annotations in Turkish, Indonesian, and Minangkabau NLP tasks. \n2. To address the practical implications of selecting between LLM-generated and human-generated annotations in the era of LLMs.",
        "Study design": "The study design is a comparative study that evaluates the quality of annotations generated by human annotators and Large Language Models (LLMs) for three NLP tasks: topic classification, tweet sentiment analysis, and emotion classification. The study uses a curated dataset and employs precision, recall, and F1-score metrics to assess the quality of the annotations.",
        "Intervention effects": "Not applicable (the paper does not report any quantitative effects of interventions)",
        "Hypotheses tested": "- To contribute to the ongoing discourse on the reliability and applicability of LLM-generated annotations in Turkish, Indonesian, and Minangkabau NLP tasks, and to scrutinize the limitations of LLMs in context comprehension, ambiguity resolution, and domain-specific nuances. \n- To address the practical implications of annotation source selection in the era of LLMs, and to understand when and where LLM-generated annotations can be confidently employed versus when human expertise remains irreplaceable.",
        "Experimental techniques": "- Calculating average accuracy of annotations by considering only instances with unanimous agreement between two human annotators \n- Assessing intercoder agreement by examining the percentage of instances where both annotators assigned the same class label \n- Employing precision, recall, and F1-score metrics to evaluate the quality of annotations \n- Using a combination of annotation methodologies: human annotation, Mechanical Turk (MTurk), and LLM (Large Language Model) annotation \n- Developing comprehensive guidelines for the unique challenges of each NLP task, considering the nuances of the target languages (Turkish, Indonesian, Minangkabau) - Leveraging large language models like ChatGPT for zero-shot annotation tasks"
    },
    "Classification of intent in moderating online discussions: An empirical evaluation": {
        "Research question": "How can we use large language models and natural language processing to moderate online discussions and identify user intent in order to foster healthier digital communication environments?",
        "Summary of introduction": "This paper investigates the use of large language models (LLMs) for moderating online discussions, with a focus on identifying user intent in various types of content, and explores content classification methods like sentiment analysis, keyword extraction, and topic modeling, while also discussing the limitations of current LLMs and providing ideas for improving model fine-tuning.",
        "Dataset": "The datasets used in the study were: \n1. The [62] comment section [63] dataset, which consists of 11,773 labeled and 1,000,000 unlabeled posts made between June 2015 and May 2016. \n2. The New York Times Comments collection [64] dataset, which consists of over 2 million comments made on articles published between January 2017 and April 2018.",
        "Limitations": "- Accuracy of the models: Risk of misclassification and mislabeling leading to false positives and negatives \n- Lack of context when classifying sentences: Implications of statements can be inherited without explicitly repeating the content \n- Bias introduced by using LLMs: The models learn from biased data and the fine-tuning process can introduce further bias \n- Inability to evaluate certain content formats: Media files, images, videos, and external websites are not evaluated in the current approach",
        "Research gaps": "- The effectiveness of freely available NLP tools for moderation tasks, as opposed to more advanced commercial solutions \n- The challenges posed by the rise of large language models (LLMs) in creating spam that is hard to detect \n- The need to fine-tune models to address platform-specific issues like spam or inappropriate language \n- The limitations of current models in accurately detecting certain types of problematic content like sarcasm, irony, and nuanced language \n- The ethical considerations and potential risks of over-reliance on automated moderation tools",
        "Software used": "Python 3.11.3, Flask, SQL database",
        "Algorithms": "Transformer architectures, BERT, TinyBERT, LoRA, LLaMA, and Stanford Alpaca",
        "Methodology": "- The methodology involves a Python application that: \n- Stores online discussion comments in a database \n- Allows customization of module selection for each dataset \n- Has modules with initialization and classification functions \n- Supports three main approaches to evaluating the data: one-by-one with all modules, one module at a time, or multiple comments/modules simultaneously",
        "Main findings": "- The paper investigates using large language models (LLMs) for moderating online discussions and identifying user intent. \n- It discusses the limitations of the approach and the output of the application, as well as ethical considerations. \n- The paper concludes with a summary of the findings and potential future research directions.",
        "Study Objectives": "- Investigate the use of large language models (LLMs) for moderating online discussions \n- Focus on identifying user intent in various types of content \n- Utilize NLP techniques to detect toxic language, derailment in discussions, and problematic comments \n- Create prototypes of such tools using LLMs \n- Evaluate the tools using datasets in English and German \n- Explore content classification through sentiment analysis, keyword extraction, and topic modeling, using non-binary labeling \n- Discuss the limitations of current LLMs, including the challenge of false positives due to limited training data \n- Provide ideas for improving model fine-tuning to better address specific platform needs and linguistic variations",
        "Study design": "Not mentioned (the paper does not explicitly state the study design)",
        "Intervention effects": "Not mentioned (the paper does not report on any specific interventions or their quantitative effects)",
        "Hypotheses tested": "Not mentioned (the paper does not state any specific hypotheses to be tested)",
        "Experimental techniques": "- Evaluating full datasets by: \n- Evaluating comments one by one with all modules in sequence \n- Analyzing all comments with one module at a time and then the next module \n- Evaluating multiple comments and/or multiple modules simultaneously using threading \n- Allowing users to tailor module selection for each dataset to optimize for accuracy and efficiency \n- Structuring the data interaction component into three segments: database management, active database selection, and dataset import and selection \n- Implementing modules with an initialization function and a classification function"
    },
    "Advancing CSR Theme and Topic Classification: LLMs and Training Enhancement Insights": {
        "Research question": "How can we effectively classify CSR themes and topics across multiple languages?",
        "Summary of introduction": "The paper presents a study on classifying corporate social responsibility (CSR) themes and topics, evaluating the performance of various machine learning models and the effectiveness of data augmentation, data translation, and contrastive learning, finding that generative LLMs in a zero-shot setup underperform compared to fine-tuned local models with enhanced datasets and additional training objectives for more complex classification tasks.",
        "Dataset": "The datasets used in the study were: \n1. A cross-lingual, multi-class dataset for CSR theme recognition \n2. A monolingual multi-label dataset for CSR topics in the Environment (ENV) theme \n3. A monolingual multi-label dataset for CSR topics in the Labour and Human Rights (LAB) theme",
        "Limitations": "- The computational cost of running the models on a large scale was not considered, which is important for practical applications due to resource constraints. \n- The choice of prompts for zero-shot classification and label interpretation may have affected the results, suggesting that exploring different prompting strategies could enhance performance. \n- Despite no significant impact observed from testing truncated texts, models capable of processing longer sequences might inherently benefit from more contextual information.",
        "Research gaps": "- The computational cost of running models on a large scale was not considered, which is crucial in practical applications due to resource constraints. \n- The choice of prompts for zero-shot classification and label interpretation may have affected the results, suggesting that exploring different prompting strategies could enhance performance. \n- Despite no significant impact observed from testing truncated texts, models capable of processing longer sequences might inherently benefit from more contextual information.",
        "Software used": "Trafilatura, GPT-3.5, Boilerpy, Multi-Layer Perceptron (MLP), LSTM",
        "Algorithms": "The specific algorithms introduced, studied, or used in the study include: Multi-Layer Perceptron (MLP), Long Short-Term Memory (LSTM), Multilingual DistilBERT, XLM-RoBERTa, XLM-RoBERTa-large, DistilBERT, BERT, RoBERTa, RoBERTa-large, and a variant of the NT-Xent contrastive loss function.",
        "Methodology": "The key methodological elements used in the study were: \n- Evaluation of a wide range of classification models, including traditional and complex models \n- Use of multilingual pre-trained language models for the themes dataset \n- Use of models like DistilBERT, BERT, RoBERTa, and RoBERTa-large for the multi-label datasets\n- Data augmentation through paraphrasing and translation to expand the training data \n- Exploration of contrastive learning to improve the performance of multi-label classification models",
        "Main findings": "- Fine-tuned language models outperformed state-of-the-art generative LLMs like GPT-3.5 and GPT-4 on the more complex multi-class and multi-label CSR classification tasks. \n- Generative LLMs in a zero-shot setup struggled with the complexities of multi-label classification, where a single instance can have multiple labels assigned to it. \n- Fine-tuning the models on the specific training data allowed them to learn the text-specific features needed for better performance on these complex classification tasks.",
        "Study Objectives": "1. Perform cross-lingual multi-class classification of CSR themes 2. Perform monolingual multi-label classification of CSR topics 3. Hypothesize that fine-tuned smaller language models can outperform generative LLMs on more complex classification tasks 4. Hypothesize that contrastive learning can improve performance 5. Hypothesize that generative LLMs can produce useful synthetic data to enhance classification model performance",
        "Study design": "Not applicable (the paper describes a text classification task, not an experimental study with a specific study design)",
        "Intervention effects": "Not mentioned (the paper does not report any quantitative intervention effects or outcomes)",
        "Hypotheses tested": "- Fine-tuning smaller language models can outperform more recent generative LLMs for more complex classification tasks. \n- Contrastive Learning can further improve performance, and generative LLMs can produce useful synthetic data to enhance the performance of classification models.",
        "Experimental techniques": "- Multi-Layer Perceptron (MLP) and LSTM models \n- Data augmentation through paraphrasing using Mixtral \n- Data translation to French and simplified Chinese using Google Translate \n- Contrastive learning using a variant of NT-Xent"
    }
}